# Projeto Integrador: Aplica√ß√µes de Intelig√™ncia Artificial

## üìö Vis√£o Geral do Projeto

Este projeto implementa e treina um agente de Intelig√™ncia Artificial utilizando o algoritmo **Q-Learning** para jogar o jogo de tabuleiro **Kamisado**. Ele explora conceitos de aprendizado por refor√ßo e compara o desempenho com um agente Minimax.

## üé≤ O Jogo Kamisado

Kamisado √© um jogo de tabuleiro 8x8 para dois jogadores. O objetivo √© mover suas pe√ßas at√© a linha de base do oponente. A regra principal √© que a cor da casa onde uma pe√ßa para determina a cor da pe√ßa que o oponente deve mover no pr√≥ximo turno. O jogo tamb√©m possui uma condi√ß√£o de empate para evitar estagna√ß√£o.

## ü§ñ Agentes de Intelig√™ncia Artificial

Exploramos dois agentes:

1.  **Agente Minimax:** Usa busca em √°rvore e uma **heur√≠stica** para encontrar a melhor jogada.
2.  **Agente Q-Learning:** Aprende a jogar por **experi√™ncia e experimenta√ß√£o**, construindo uma **Tabela Q**.

## üíª Estrutura do C√≥digo

O c√≥digo √© organizado em fun√ß√µes e classes para representar o jogo e os agentes:

* Representa√ß√£o do tabuleiro e estado (`encode_state`).
* Fun√ß√µes para movimentos (`successors`) e fim de jogo (`game_over`).
* Classes `QLearningAgent` e `MinimaxAgent`.
* L√≥gica de treinamento (`train_against_minimax`) com curr√≠culo.
* Menu interativo para execu√ß√£o.

## üîó Materiais da Apresenta√ß√£o

* **Slides da Apresenta√ß√£o:** [Kamisado Q-Learning Slides](https://www.canva.com/design/DAGqPu_Dvq8/JGwVtBlIk1j-1hismn5R4w/edit?utm_content=DAGqPu_Dvq8&utm_campaign=designshare&utm_medium=link2&utm_source=sharebutton)
* **V√≠deo da Apresenta√ß√£o:** [Assista ao V√≠deo no YouTube](https://youtu.be/K9RebnOcXds)
* **Notebook no Google Colab:** [Acesse o Projeto no Google Colab](https://colab.research.google.com/drive/1uN5An6r4L0KLboHrVVgS3eeuVc0CRMnH?usp=sharing)


## üìä Resultados e Desafios

O agente Q-Learning foi treinado por um total de **10.000** epis√≥dios contra um Minimax de profundidade crescente.

* **Desempenho Final (Estat√≠sticas de Treinamento - Primeiro Ciclo de 10.000 epis√≥dios):**
    * Vit√≥rias (como Pretas): **31**
    * Derrotas (como Pretas): **51**
    * Empates (estat√≠stica interna): **0** (conforme registrado na ferramenta de stats, mas muitos jogos resultaram em estagna√ß√£o).
    * Taxa de vit√≥rias do Q-Agent: **37.8%** (com base apenas em vit√≥rias e derrotas n√£o-empate).
    * N√∫mero total de estados-a√ß√£o aprendidos: **570** Q-entradas.

* **Avalia√ß√£o de Desempenho (100 Partidas Q-Learning vs Minimax):**
    * Q-Agent (vit√≥rias): **0/100**
    * Minimax (vit√≥rias): **0/100**
    * Empates: **100/100**
    * **An√°lise:** A avalia√ß√£o consistente em empates demonstra a dificuldade do agente em converter o aprendizado em vit√≥rias diretas contra um Minimax, e a preval√™ncia de estagna√ß√£o no jogo.

* **Desafios Chave:**
    * **Explora√ß√£o Limitada:** Atingir um n√∫mero maior de estados-a√ß√£o visitados mostrou-se um desafio, impactando o aprendizado global.
    * **Custo Computacional:** A complexidade da busca Minimax (especialmente em profundidade 3) resultou em tempos de treinamento prolongados (aproximadamente 20 minutos por 1000 epis√≥dios na fase final).
    * **Converg√™ncia:** A converg√™ncia para um desempenho superior em jogos complexos de alto espa√ßo de estados, como o Kamisado, demanda um volume de treinamento excepcionalmente grande (centenas de milhares ou milh√µes de epis√≥dios), que excedeu o escopo e o tempo deste projeto.

* **Solu√ß√µes Aplicadas:** Implementamos ajustes nos hiperpar√¢metros de explora√ß√£o (aumentando `EPSILON_MIN` e diminuindo `EPSILON_DECAY`), na fun√ß√£o de recompensa (penalidades mais agressivas por empate e por turno) e no curr√≠culo de treinamento (profundidade do Minimax gradual) para tentar otimizar o aprendizado e a explora√ß√£o do ambiente.

## üí° Considera√ß√µes Finais

Este projeto demonstrou a implementa√ß√£o funcional de um agente de Intelig√™ncia Artificial baseado em **Q-Learning** para o jogo **Kamisado**, interagindo com um oponente **Minimax**.

A experi√™ncia pr√°tica permitiu uma compreens√£o aprofundada de conceitos como representa√ß√£o de estado, balanceamento entre explora√ß√£o/explota√ß√£o e calibra√ß√£o de recompensas e hiperpar√¢metros.

A principal conclus√£o do trabalho √© que, apesar do aprendizado observado e dos ajustes realizados, a converg√™ncia para um desempenho de vit√≥ria consistente em jogos complexos como o Kamisado exige um **volume de treinamento excepcionalmente grande (centenas de milhares ou milh√µes de epis√≥dios)**. Isso ressalta os desafios de escala do Aprendizado por Refor√ßo, demonstrando a necessidade de vastos recursos computacionais e tempo para otimizar agentes em ambientes complexos.

---
